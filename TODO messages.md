A large amount of data is output on the 5th outlet with messages. The format is determined by the value of the`@skeletonformat` attribute which chooses between an OSC format or a native Max route-friendly format. For example:

    OSC                         Max route
    ----------------------------------------------
    /user/4 0.3 1.1 1.9         user 4 0.3 1.1 1.9

The OSC format on the left can be queried and manipulated with OSC tools like OSC-route from [CNMAT](http://cnmat.berkeley.edu/downloads/externals/overview). The native Max format on the right can be manipulated with built-in objects like route, routepass, and select.

### User Identification #
User identification messages are generated by enabling `@skeleton`.  The data provided is a long integer "playerID" which is a unique identifier representing the currently tracked/lost person. This ID is unique within a given point of time and for a person seen by the Kinect. Its value for Kinect v1 hardware is in the inclusive range [1...6]. This same playerID is used for matching/connecting with other Kinect data like the person's location, joints, and depthmap pixels.

The three messages output are as follows:

    OSC                        Max route                
    -------------------------------------------------
    /new_user playerID         new_user playerID
    /lost_user playerID        lost_user playerID
    /calib_success playerID    calib_success playerID

Examples:

    /new_user 2                new_user 2
    /calib_success 2           calib_success 2
    /lost_user 2               lost_user 2

`new_user` Indicates a new person has been identified but full skeleton tracking is not yet available. This is the first message in the possible progression of events for a given playerId.

`calib_success` Indicates the person's skeleton joints are now being fully tracked. This can only be received after a `new_user` message.

`lost_user` Indicates the previously identified/tracked person has been lost. This can only be received after a `new_user` message. It is possible to receive this event and _not_ have received a `calib_success` message. This would indicate that a person was seen but the Kinect was not able to fully track them before they were lost. This lack of full skeletal tracking could be because of interference or because the Kinect was already skeletal tracking its maximum number of people. After receiving this message, the playerId may be reused by the Kinect for a new person.

If you are migrating from jit.openni, the additional messages calib_pose, calib_start, calib_fail, exit_user, and reenter_user are not supported.

### User Location #
User location messages, also known as center of mass, are generated when `@skeleton` is enabled. Once a person is identified, you will begin receiving user location messages. It is not necessary to have full skeletal tracking as indicated by a `calib_success` message. This enables you to roughly track a person's location when there is interference or because the Kinect was already skeletal tracking its maximum number of people.

    OSC                          Max route
    --------------------------------------------------
    /user/playerID x y z q       user playerID x y z q

* `userid` is an long integer and is the same unique playerID from the matching user identification message
* `x, y, z` are floating point numbers representing the real-world location of the person's center of mass relative to the Kinect sensor. Their values are influenced by the attributes `@distmeter`, `@flipx`, `@rotatemethod`, and `@translate`.
* q is an integer bit field that indicates the "quality" of the person's location. It is possible to have no, one, or multiple quality flags at the same time. The bit flags are summed together.
  Quality bitflag values are:  
  1 = Part of the player's body is out of frame to the camera's right.  
  2 = Part of the player's body is out of frame to the camera's left.  
  4 = Part of the player's body is out of frame above the camera's field of view.  
  8 = Part of the player's body is out of frame below the camera's field of view.  
  _Example:_ quality = 5 indicates the player's body is out of frame to the camera's right (1) and above (4). 1 + 4 = 5.

Examples:

    OSC                                       Max route
    -----------------------------------------------------------------------------
    /user/2 42.1323 -100.8237 1984.348 0      user 2 42.1323 -100.8237 1984.348 0

### Skeleton Joints #
Skeleton joint data is generated when `@skeleton`, `@skeldepth`, or `@skelcolor` are enabled. This data is affected by the other attributes `@distmeter, @flipx, @align, @position, @quat, @rotate, @rotatexyz, and @scale`. More info about joint positions is available at <https://docs.microsoft.com/en-us/previous-versions/windows/kinect/dn799273(v=ieb.10)> and <https://docs.microsoft.com/en-us/previous-versions/windows/kinect-1.8/jj131025(v=ieb.10)#skeleton-position-and-tracking-state> (the Kinect v1 documentation is better than v2).

The Microsoft default behavior for skeleton joint tracking will select the six most engaged users in the field of view. This behavior is consistent (not random), yet at the same time not driven by specific criteria. You can not manually select users for skeletal tracking.

Your choice for the basic message format is controlled with `@skeletonformat`. You can choose between

* `@skeletonformat 0` OSC: this format may be helpful when sending raw Kinect data to heterogeneous network devices
* `@skeletonformat 1` Max-native (route): usually better when transforming Kinect data within Cycling74 Max.

#### Skeleton Space Joints #

Orientation format choice is controlled with `@orientformat` and described in its own section below.

```sh
# OSC:                            @skeleton 1
/skel/userid/jointname x y z confidence

# OSC + orientation:              @skeleton 2
/skel/userid/jointname x y z confidence qx qy qz qw

# Max native/route:               @skeleton 1 @skeletonformat 1
skel userid jointname x y z confidence

# Max native/route + orientation: @skeleton 2 @skeletonformat 1
skel userid jointname x y z confidence qx qy qz qw
```

* `userid` is an integer which represents a user id, the same user/player id as described in previous sections
* `jointname` is one of the strings listed below; these are the same joint names as used by jit.openni
* `x, y, z` are floating point numbers and their values are influenced by `@distmeter, @flipx, @align, @position, @quat, @rotate, @rotatexyz, and @scale`
* `confidence` is a floating point number representing the sensor's confidence in the data values. The confidence value is [0.0, 0.5, 1.0] with 1.0 indicating the highest confidence. This confidence can be used to filter data by your patch or automatically with the [@posconfidence](Attributes#visual-adjustment) attribute.

Supported `jointname` for all output formats are:

    head        neck     torso    waist
    l_shoulder  l_elbow  l_wrist  l_hand
    r_shoulder  r_elbow  r_wrist  r_hand
    l_hip       l_knee   l_ankle  l_foot
    r_hip       r_knee   r_ankle  r_foot

Examples

```sh
# OSC:                            @skeleton 1 @distmeter 1
/skel/2/r_shoulder -1.204 2.053 3.712 0.5

# OSC + orientation:              @skeleton 2 @distmeter 1
/skel/2/r_shoulder -1.204 2.053 3.712 0.5 0.586775 0.469815 0.567755 -0.335593

# Max native/route:               @skeleton 1 @distmeter 1 @skeletonformat 1
skel 2 r_shoulder -1.204 2.053 3.712 0.5

# Max native/route + orientation: @skeleton 2 @distmeter 1 @skeletonformat 1
skel 2 r_shoulder -1.204 2.053 3.712 0.5 0.586775 0.469815 0.567755 -0.335593
```

#### Skeleton Space Real-world Orientation #

The orientation of real-world coordinates can be optionally enabled with `@skeleton 2`. It is output in your choice of four formats using `@orientformat`. The output format and values of the orientation data are different than with jit.openni. You will need to account for this in your patch logic/math.

```sh
# Max native + quaternion orientation: @skeleton 2 @skeletonformat 1 @orientformat 0
skel userid jointname x y z confidence qx qy qz qw

# Max native + quaternion orientation: @skeleton 2 @skeletonformat 1 @orientformat 2
skel userid jointname x y z confidence m11 m12 m13 m14 m21 m22 m23 m24 m31 m32 m33 m34 m41 m42 m43 m44
```

This orientation data is output in your choice of a quaternion (4 floats) or 4x4 rotation matrix (16 floats). If you were to look at the 3x3 rotation matrix within the 4x4, the first three are the direction of the joint's +X axis given as a 3-vector in the chosen coordinate space, the second three are the +Y axis direction, and the third three are the +Z axis direction.

In addition, the coordinate space for this orientation data is in your choice of *absolute* world coordinate space or in a *hierarchical* rotation coordinate space. This is different than OpenNI (which jit.openni uses) where rotations are relative to a T pose.

Details on the orientation data can be found at <https://docs.microsoft.com/en-us/previous-versions/windows/kinect/dn799273(v=ieb.10)#joint-normals> and <https://docs.microsoft.com/en-us/previous-versions/windows/kinect-1.8/hh973073(v=ieb.10)> (the Kinect v1 documentation is better than v2).

#### Color Space Joints #

Color space joint messages do not provide depth or orientation.

```sh
# @skeletonformat 0
/skelcolor/userid/jointname column row confidence

# @skeletonformat 1
skelcolor userid jointname column row confidence
```

* `userid`, `jointname`, and `confidence` are the same as written above in skeleton space
* `column, row` are long integers that represent the column and row in the colormap image

Examples

```sh
# @skeletonformat 0
/skelcolor/2/r_shoulder 975 758 0.5

# @skeletonformat 1
skelcolor 2 r_shoulder 975 758 0.5
```

#### Depth Space Joints #

Depth space joint messages do not provide orientation.

```sh
# @skeletonformat 0
/skeldepth/userid/jointname column row depth confidence

# @skeletonformat 1
skeldepth userid jointname column row depth confidence
```

* `userid`, `jointname`, and `confidence` are the same as written above in skeleton space
* `column, row` are long integers that represent the column and row in the depthmap image
* `depth` is the depth at that column, row pixel

Examples

```sh
# @skeletonformat 0 @distmeter 0
/skeldepth/2/r_shoulder 105 342 3712 0.5

# @skeletonformat 1 @distmeter 1
skeldepth 2 r_shoulder 105 342 3.712 0.5 
```

### Body Properties # 
`@bodyprop=1` will enable output of body properties: restricted, handstate, and lean. These properties provide information about the appearance or state of a tracked body and the confidence of that result. The results and confidence may vary. You may need to filter or smooth them. Note, `@flipx` does not affect the lean values. Max route friendly @skeletonformat=1 is also supported.

    OSC                                           Max route  
    ------------------------------------------------------------------    
    /restricted/id isrestricted confidence        restricted id isrestricted confidence
    /handstate/id/left thehandstate confidence    handstate id left thehandstate confidence
    /handstate/id/right state confidence          handstate id right state confidence
    /lean/id lean_x lean_y confidence             lean id lean_x lean_y confidence

* `id` is a long integer matching a tracked skeleton
* `isrestricted` is an integer (0=false, 1=true) representing if the body is restricted from a full range of motion
* `thehandstate` is a string representing one of the five [hand states](http://msdn.microsoft.com/en-us/library/dn799273#ID4EOC) of: unknown, nottracked, open, closed, lasso
* `lean_x` and `lean_y` are floats representing [left, right, forward, and backward leaning](http://msdn.microsoft.com/en-us/library/dn785526.aspx) of the body. The values range between -1.0 (left or back) and 1.0 (right or front) in both directions, where 1.0 roughly corresponds to 45 degrees of lean.
* `confidence` is a floating point number representing the confidence result. The value is [0.0, 0.5, 1.0] with 1.0 indicating the highest confidence

For example:
* `/restricted/2 1 1.0` skeleton 2 appears restricted from full motion and Kinect is very confident
* `/handstate/2/left open 1.0` left hand on skeleton 2 appears to be open and Kinect is very confident
* `/handstate/2/right lasso 0.5` right hand on skeleton 2 appears to be a lasso and Kinect is only somewhat confident
* `/lean/2 0.2 0.3 1.0` skeleton 2 is leaning slightly right, slightly forward, and Kinect is very confident
* `/lean/2 -0.5 0.0 0.5` skeleton 2 is leaning left and Kinect is only somewhat confident

### Floor #
Floor identification occurs when `@skeleton` is enabled and the values stabilize over time. Once the floor is identified by the Kinect, you will begin receiving floor messages if `@flooronbang` is enabled. The floor values are influenced by the `@distmeters` and `@flipx` attributes.

    OSC                     Max route  
    -----------------------------------------  
    /floor x y z i j k      floor x y z i j k

* `x, y, z` are floating point numbers and are coordinates for a point on the plane
* `i, j, k` are floating point numbers and are of a vector normal to the plane
* Natively, the Kinect SDK describes the clipping plane as the equation Ax + By + Cz + D = 0. Given the floor output of 6 floats, you can derive the native values:  
A = the i value given your choice of `@flipx`  
B = the j value  
C = the k value  
D = negative of y value in meters or mm

Examples:

    OSC                                     Max route  
    ----------------------------------------------------------------------------  
    /floor 0.0 -925.7 0.0 0.12 0.97 0.047   floor 0.0 -925.7 0.0 0.12 0.97 0.047

An example of using this data is to position a jit.gl.gridshape plane using a jit.anim.node. Connect the node to the gridshape. Then set the `@position` of the node to be the `x, y, z` values and the `@direction` of the node to be the `i, j, k` values.

It is not necessary to enable `@flooronbang` to get this floor data in a message. These same values are available from the `@floor` attribute. It can be queried using the standard Max mechanism of prepending "get" on the attribute, `getfloor`, and sending that message to the 1st inlet. Its result will be output via the dumpout outlet. 

### Face Tracking #
Face tracking occurs when `@faces` is enabled with a value of `1`. The Kinect v2 supports tracking faces on all tracked skeletons. Data for face tracking is output on the OSC/route message outlet and most is affected by `@flipx`, `@distmeter` and `@skeletonformat` values; where they are not, it will be specifically noted. Face data is not rotated by gravity or elevation; you will need to do that yourself with jit.anim.node or custom calculations.

**Some face features (shape units, hidef 2D points, 3D model, and face colors) require a fully captured/modeled face. This full capture is: a) drastically slow compared to the Kinect v1; b) large facial hair tends to interfere; and c) requires the face to be rotated slowly in four directions. The status of this capture can be seen with the `modelstatus` message.**

`@faces=1` will always output the bounding box around the face in color/depth pixel coordinates and the 3D pose of the face in the real-world location. You must enable this or all other face tracking options will be ignored.

    OSC                                                  Max route  
    -----------------------------------------------------------------------------------------------------  
    /face/id/bounds left top right bottom                face id bounds left top right bottom  
    /face/id/boundsdepth left top right bottom           face id boundsdepth left top right bottom  
    /face/id/pose scale xrot yrot zrot xtrn ytrn ztrn    face id pose scale xrot yrot zrot xtrn ytrn ztrn  
    /face/id/modelstatus collection capture              face id modelstatus collection capture

* `bounds` has color coordinates and `boundsdepth` has depth coordinates
* `left` is a long integer that represents the leftmost X coordinate of the bounding box
* `top` is a long integer that represents the topmost Y coordinate of the bounding box
* `right` is a long integer that represents the rightmost X coordinate of the bounding box
* `bottom` is a long integer that represents the bottomost Y coordinate of the bounding box
* `scale` is a float where 1.0 means that it is equal in size to the loaded 3D model (in the model space)
* `xrot`, `yrot`, `zrot` are floats of Euler rotation angles in degrees of rotation around X, Y, and Z axis
* `xtrn`, `ytrn`, `ztrn` are floats of 3D translations on the X, Y, and Z axis in real-world space
* `collection` is a long integer bitfield that represents the [collection needs](https://msdn.microsoft.com/en-us/library/microsoft.kinect.face.facemodelbuildercollectionstatus.aspx)
* `capture` is a long integer that represents the [capture status](https://msdn.microsoft.com/en-us/library/microsoft.kinect.face.facemodelbuildercapturestatus.aspx)

**2D Face Points**  
`@face2dpoints` enables output of 2D points in color camera coordinate space that describe the face. It is one of the following values: 0=disabled, 1=Kinect v1 hidef points, 2=Kinect v2 basic points, 3=Kinect v2 hidef points. At this time, `1=Kinect v1 hidef points` are not available on dp.kinect2. Only values 2 and 3 are available.

* 2: Kinect v2 basic points are the (x,y) coordinates of [these 5 basic face points](http://msdn.microsoft.com/en-us/library/microsoft.kinect.face.facepointtype.aspx) listed in the same order.  
* 3: Kinect v2 hidef points are the (x,y) coordinates of [these defailed face points](http://msdn.microsoft.com/en-us/library/microsoft.kinect.face.highdetailfacepoints.aspx) listed in the same order. These require a fully captured face; see `modelstatus` message.

The output of OSC is below with the Max route format `@skeletonformat=1` following the same pattern as previous examples.

    OSC below (@skeletonformat=1 is also supported)    Max route
    ----------------------------------------------------------------------------------------------  
    /face/id/points2d x1 y1 x2 y2 x3 y3 ...            face id points2d x1 y1 x2 y2 x3 y3 ...
    /face/id/points2ddepth x1 y1 x2 y2 x3 y3 ...       face id points2ddepth x1 y1 x2 y2 x3 y3 ...

* `points2d` has color coordinates and `points2ddepth` has depth coordinates
* `x1` and `y1` are long integers and represent the x,y coordinate in color space of point 1 of the face
* `x2` and `y2` are long integers and represent the x,y coordinate in color space of point 2 of the face
* As an example, if you select `2=Kinect v2 basic points` there are 5 points, therefore 10 integers will be output

**Face Shape and Animation Units**  
`@facesuau` will enable animation unit (AU) and shape unit (SU) output for tracked faces. There are three values: `0=disabled, 1=Kinectv1 AU/SU, 2=Kinectv2 AU/SU.` You can smooth these values with `@facesuausmooth`. The output format is the same with dp.kinect and dp.kinect2. _However_, the meaning of the v1 and v2 AU/SUs are different. [v1 is documented](https://github.com/diablodale/dp.kinect/wiki/Message-based-Data#face-tracking) with dp.kinect.  Microsoft did not make the Kinect v2 backwards compatible with v1 and they did not follow the Candide-3 face model. The model with the Kinect v2 and therefore with dp.kinect2 is a Microsoft proprietary model. At this time, there is only brief Microsoft documentation:

* [Animation Units for Kinect v2](http://msdn.microsoft.com/en-us/library/microsoft.kinect.face.faceshapeanimations.aspx)
* [Shape Units for Kinect v2](http://msdn.microsoft.com/en-us/library/microsoft.kinect.face.faceshapedeformations.aspx) require a fully captured face; see `modelstatus` message

Output examples

    OSC (@skeletonformat=0)  
    -----------------------------------------------  
    /face/id/shapeunits converged scale su1 su2 ...  
    /face/id/animunits au1 au2 ...  
      
    Max route (@skeletonformat=1)  
    ----------------------------------------------  
    face id shapeunits converged scale su1 su2 ...  
    face id animunits au1 au2 ...


* `converged` is a long integer [0=false,1=true] when shape unit coefficients have converged to realistic values
* `scale` is a float where 1.0 means that it is equal in size to the loaded 3D model (in the model space).
* su1, su2, ... are floats representing the respective SUs described in the MSDN documentation
* au1, au2, ... are floats representing the respective AUs described in the MSDN documentation

**3D Face Model**  
`@face3dmodel = 3, 4, 7, or 8` will enable output of a 3D model of the face in local face coordinate space. Therefore, the `@distmeter` does not apply to this set of data. This model and its coordinate space can be scaled, translated, and rotated using the 3D `pose` values described above. The 3D face models require a fully captured face; see `modelstatus` message.

[![dp.kinect2 face model render](https://cloud.githubusercontent.com/assets/679350/20496090/0d505f86-b024-11e6-9dbf-a9007afbc6c7.png)](http://hidale.com/dpkinect2face/) Microsoft provides [little documentation](https://msdn.microsoft.com/en-us/library/dn785525.aspx#ID4EXC) on this feature and cautions in their forums that the 3D face model could change in future SDK updates. For now with SDK 2.0 on which dp.kinect2 is written, there are 1347 vertices making 2630 triangles.

This 3D face data is available in several formats:

3) `@face3dmodel=3` will output the `triangles` list describing counterclockwise triangles. Each triangle is a group of 9 values representing three 3D vertices; each vertex being an xyz coordinate
   ```
   OSC below (@skeletonformat=1 is also supported)
   --------------------------------------------------------------------    
   /face/id/triangles t1_v1_x t1_v1_y t1_v1_z t1_v2_x t1_v2_y t1_v2_z t1_v3_x t1_v3_y t1_v3_z ...
   ```
   * `t1_v1_x`, `t1_v1_y`, `t1_v1_z` are floats representing the x, y, z coordinates in local face coordinate space for the first vertex of the triangle
   * `t1_v2_x`, `t1_v2_y`, `t1_v2_z` are floats representing the x, y, z coordinates in local face coordinate space for the second vertex of the triangle
   * `t1_v3_x`, `t1_v3_y`, `t1_v3_z` are floats representing the x, y, z coordinates in local face coordinate space for the third vertex of the triangle
   * the above grouping of 9 repeats to describe all triangles in the model

4) `@face3dmodel=4` will output a long list of indices and then a long list of 3D points in that index order. Each triangle is described by three index values representing the three vertices in counterclockwise order. The index value is used as a lookup to find the index'th 3D point in the second list. The second list is a series of 3D points; each point being an xyz coordinate. A single 3D point will likely be referenced by its index more than one time because the triangles in the face model will share vertices.
   ```
   OSC below (@skeletonformat=1 is also supported)  
   --------------------------------------------------------------------  
   /face/id/indices t1_v1_idx t1_v2_idx t1_v3_idx t2_v1_idx t2_v2_idx t2_v3_idx ...  
   /face/id/vertices v1_x v1_y v1_z v2_x v2_y v2_z ...
   ```
   * `t1_v1_idx`, `t1_v2_idx`, `t1_v3_idx` are long integers representing the index lookup for the first, second, and third 3D point for each vertex of the first triangle
   * `t2_v1_idx`, `t2_v2_idx`, `t2_v3_idx` are long integers representing the index lookup for the first, second, and third 3d point for each vertex of the second triangle
   * The above pattern repeats until all triangles in the model are described with each their three indices
   * `v1_x`, `v1_y`, `v1_z` represent the x, y, z coordinate for a 3D point in local face coordinate space
   * `v2_x`, `v2_y`, `v2_z` represent the x, y, z coordinate for another 3D point in local face coordinate space
   * The above pattern of 3D points is repeated until all 3D points needed to create the model are output.
5) Not supported on the Kinect v2.
6) Not supported on the Kinect v2.
7) `@face3dmodel=7` will output a single 3-plane float32 matrix describing counterclockwise triangles with each triangle defined by sequential groups of three cells (xyz as the three planes) of the matrix representing the triangle's three 3D vertices. Other than it being stored in a matrix, the data is the same as (3) above.

8) `@face3dmodel=8` will output two matrices; first the indices used as a lookup to find the index'th 3D point of the second matrix to create triangles. The second matrix is a 3-plane float32 matrix with each cell describing an xyz point in 3D space. This may be a more efficient format than (7) because the indices matrix will only be sent once since the triangle relationships do not change. The 3D points matrix will update often since the vertices' xyz position on a face changes with every movement. A single 3D point will likely be referenced by its index more than one time because the triangles in the face model will share vertices. Other than it being stored in a matrix, the data is the same as (4) above.

**Face Colors (skin and hair)**  
`@facecolors=1` will enable output of face skin and hair color. Microsoft's technology derives the face and hair color by building a complete 3D model for the face and understanding the colors of that model. This 3D model process is described above.

    OSC below (max route friendly @skeletonformat=1 is also supported)
    --------------------------------------------------------------------    
    /face/id/haircolor red green blue alpha
    /face/id/skincolor red green blue alpha

* `id` is a long integer matching a tracked skeleton
* `red` is a float32 representing the red component of the color
* `green` is a float32 representing the green component of the color
* `blue` is a float32 representing the blue component of the color
* `alpha` is a float32 representing the alpha component of the color. It is always 1.0.

For example:
* `/face/2/haircolor 0.246 0.845 0.472 1.0` face on skeleton 2 has haircolor of red=0.246, green=0.845, blue=0.472
* `/face/5/skincolor 0.114 0.619 0.247 1.0` face on skeleton 5 has skincolor of red=0.114, green=0.619, blue=0.247

**Face Properties**  
`@faceprop=1` will enable output of [face properties](http://msdn.microsoft.com/en-us/library/dn782034.aspx#ID4EID) that provide information about the appearance or state of a user's face and the confidence of that result. The results and confidence may vary. You may need to filter or smooth them. This output may be moved into a separate attribute to optimize resources before the final release.

    OSC below (max route friendly @skeletonformat=1 is also supported)
    --------------------------------------------------------------------    
    /face/id/property result confidence

* `id` is a long integer matching a tracked skeleton
* `property` is a string representing one of the eight [face properties](http://msdn.microsoft.com/en-us/library/dn782034.aspx#ID4EID) tracked
* `result` is an integer (0=false, 1=true) representing the detected state of the property
* `confidence` is a floating point number representing the confidence result. The value is [0.0, 0.5, 1.0] with 1.0 indicating the highest confidence

For example:
* `/face/2/happy 1 1.0` face on skeleton 2 expresses happiness and Kinect is very confident
* `/face/2/happy 1 0.5` face on skeleton 2 may express happiness and Kinect is only somewhat confident
* `/face/2/happy 0 1.0` face on skeleton 2 does not express happiness and Kinect is very confident
* `/face/2/lefteyeclosed 1 1.0` face on skeleton 2 has the left eye closed and Kinect is very confident
* `/face/2/lefteyeclosed 0 1.0` face on skeleton 2 has the left eye open and Kinect is very confident
* `/face/2/lookingaway 1 1.0` face on skeleton 2 is looking away and Kinect is very confident
* `/face/2/lookingaway 1 0.5` face on skeleton 2 is probably partially looking away and Kinect is only somewhat confident

### Sound Information #
Information about the sound heard by the Kinect's array microphone occurs by enabling `@soundinfooutput` to one of its two active values. If set to `2 = on bang + attribute`, then the data is output as a message on every bang.

    OSC                             Max route
    -----------------------------------------------------------
    /soundinfo energy angle conf    soundinfo energy angle conf

* `energy` is a floating point number from [0.0 - 1.0] representing the energy/loudness of the current sound heard
* `angle` is a floating point number representing the estimated horizontal angle (in degrees) to the sound heard in camera coordinates, where the x- and z-axes define the horizontal plane. The angle is relative to the z-axis, which is perpendicular to the Kinect sensor.
* `conf` is a floating point number representing the confidence value of the estimated angle. The value is in the range [0.0 - 1.0], with 1.0 indicating the highest confidence.

Examples:

    OSC                          Max route  
    -----------------------------------------------------  
    /soundinfo 0.673 25.0 0.8    soundinfo 0.673 25.0 0.8

When `@soundinfooutput=2` and `@skeleton > 0`, dp.kinect2 will additionally output a `soundbodies` message indicating a correlation between heard sound and tracked bodies. This message will list all tracked bodies which the Kinect calculates are correlated with the heard sound. You might use this information to determine who is engaging with the Kinect or who is speaking.

    OSC below (max route friendly @skeletonformat=1 is also supported)  
    --------------------------------------------------------------------    
    /soundbodies BodyID1 BodyID2 BodyID3 ...

* `BodyIDx...` are integers which indicate tracked bodies which are correlated with the heard sound

For example:
* `/soundbodies 4` tracked body #4 is correlated with the heard sound
* `/soundbodies 3 1` tracked bodies #3 and #1 are correlated with the heard sound
* `/soundbodies 1 2 3 4 5 6` tracked bodies #1, #2, #3, #4, #5, and #6 are correlated with the heard sound

### Speech Recognition #
The Kinect's speech recognition technology is optimized for command-response usage. It does not support dictation. The great performance of the sensorâ€™s microphone array allows the Kinect to localize the speaker and hear them without wearing a headset.

The speech recognition grammar, the words to recognize, are defined in a grammar XML file. This is an industry standard [GRXML file](http://msdn.microsoft.com/en-us/library/hh361658%28v=office.14%29.aspx) as defined by the [W3C standards](http://www.w3.org/TR/speech-grammar/). The Kinect does support multiple languages. US English is installed with the standard Kinect drivers. You can [download additional language packs](http://go.microsoft.com/fwlink/?LinkID=248679). See the included demo patcher with dp.kinect2 for an example of usage in the subpatch.

    OSC                                                      Max route
    -----------------------------------------------------------------------------------------------------------------
    /speech/recognize/ruleid tag conf srconf heardwords    speech recognize ruleid tag conf srconf heardwords 
    /speech/error/interfere problem                        speech error interfere problem

* `ruleid` is the value of the id attribute on the \<rule\> tag within the GRXML file
* `tag` is the text between the \<tag\> tags within the GRXML file
* `conf` is a floating point number [0.0, 0.5, 1.0] representing the increasing confidence in recognition. This confidence is used for filtering via the `@speechconf` attribute.
* `srconf` is a floating point number inclusively [0.0 - 1.0] specific to the Kinect recognition technology. It can be used for more detailed confidence information.
* `heardwords` is a single symbol containing the entire phrase recognized and matched to a `tag`. If you defined an \<item\> in your GRXML file which contained several words in a single phrase to be recognized, then `heardwords` will be a single symbol containing all of those defined words separated by a space character.
* `problem` is a single symbol representing the interference to speech recognition. These error messages are optional and only enabled when `@speech = 2 (recognition+interference)`.

The possible symbol values for interference problems are:
* `noise` The sound received is interpreted by the speech recognition engine as noise.
* `nosignal` A sound is received but it is of a constant intensity. This also includes the microphone being unplugged or muted.
* `tooloud` A sound is received but the intensity is too high for recognition. If you receive many of these events, it could be due to a known issue with the Kinect's default audio gain settings. Try enabling the `@autogain` attribute. Or, for more explicit fine-tuned control, open the Windows Control Panel and select Sound, then the Recording tab. Select Kinect Microphone Array and then press the Properties button. Select the Levels tab. Set the Microphone Array gain level to a lower value. Press OK and then OK.
* `tooquiet` A sound is received but the intensity is too low for recognition.
* `toofast` The words are spoken too quickly for recognition.
* `tooslow` The words are spoken too slowly and indicates excessive time between words.